## LinerRegression
### 模型参数
```python
class sklean.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)
```
* fit_intercept:指定是否需要计算b值
* normalize:样本在做回归前要不要做归一化
* copy_X:复制训练数据
* n_jobs:学习任务的并行度
### 属性：
* coef_:学习到的权重向量
* intercept_: b值
### 正则化
当训练数据不是满秩的时候，存在多个解析解（其实就是数据列之间有很强的关联性）。常见的做法是引入正则化，常见的正则化有：
* 岭回归，加入的是二阶范数正则化
* Lasso回归，加入的是一范数正则化
* Elastic Net,加入的是一阶二阶范数混合的正则化
## Ridge Regression
### 模型参数
```python
class sklean.liner_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, slover='auto', random_state=None)
```
* solver：求解指定最优化问题的算法
    - auto:自动选择
    - svd，使用svd分解计算岭回归系数（？？？？？）
    - cholesky，使用scipy.linalg.solve函数求解
    - sparse_cg,使用scipy.sparse.linalg.cg函数求解
    - lsqr，使用scipy.linalg.lsqr函数求解
    - sag,使用Stochastic Average Gradient descent算法
* tol 判断迭代收敛的阈值
### 属性
* n_iter_，实际迭代的次数
## 线性判别分析
```python
class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(slover='svd', shrinkage=None, priors=None,n_components=None,store_covariance=False, tol=0.0001)
```
* slover:优化算法
    - svd 奇异值分解
    - lsqr 最小平方差算法
    - eigen 特征值分解
* shrikage 削减参数
* priors：每个类别的先验概率
* n_components：指定数据降维后的维度
* store_covariance:计算协方差矩阵
* tol: svd算法中迭代收敛阈值

## 决策树
决策树算法的整个过程在于：
* 特征选择
* 决策树生成
* 决策树剪枝
### 特征选择算法
- ID3,使用信息增益来选取切分维度
- C4.5，使用信息增益比
- 基尼不纯度
- 方差，用于分类回归
### CART分类数，分类与回归树
* CART回归树用平方误差最小化策略，难点在于如何寻找切分点
* CART分类生成数采用基尼指数最小化策略
### Sklean中有两种决策树算法
* 回归决策树,DecisionTreeRegressor
* 分类决策树，DecisionTreeClassifier

## 贝叶斯分类器
